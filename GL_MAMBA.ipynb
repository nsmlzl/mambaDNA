{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98235891-ea23-416a-b32d-58467e59d458",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Dataloader (pretraining)\n",
    "* [HyenaDNA HG38 dataloader](https://github.com/HazyResearch/hyena-dna/blob/main/src/dataloaders/datasets/hg38_dataset.py)\n",
    "* HyenaDNA used training/validation intervals from *Effective gene expression prediction from sequence by integrating long-range interactions.* paper.\n",
    "\n",
    "## Tokenizer?\n",
    "* Need to check HyenaDNA; I think their Jupyter contained some code of their tokenizer\n",
    "\n",
    "## Model\n",
    "* [Original MAMBA repo](https://github.com/state-spaces/mamba)\n",
    "    * [benchmark_generation_mamba_simple.py](https://github.com/state-spaces/mamba/blob/main/benchmarks/benchmark_generation_mamba_simple.py)\n",
    "    * Uses [mambaLMHeadModel](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L173) form `mixer_seq_simple.py`\n",
    "    * Uses [MixerModel](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L83)\n",
    "    * Uses [create_block](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L21)\n",
    "    * Uses [Block](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L298) and [MAMBA](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L34) classes (from `mamba_simple.py`)\n",
    "        * Actual MAMBA operation: [mamba_inner_fn](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/ops/selective_scan_interface.py#L155)\n",
    "* [Mamba small benchmark repo](https://github.com/apapiu/mamba_small_bench)\n",
    "* [SimplerMambaSSM Jupyter Notebook](./SimplerMambaSSM.ipynb)\n",
    "    * Use mamba-ssm library\n",
    "    * See class BigNeuralNetwork\n",
    "* [MAMBA chat](https://github.com/havenhq/mamba-chat/blob/main/train_mamba.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75af3307-1af8-4faf-b32b-99de0382706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mamba_ssm import Mamba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec1144b-3e2a-4cbd-9a73-db90afc31f48",
   "metadata": {},
   "source": [
    "# Download genetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09f33b3a-1046-4a84-9d74-4cf1d5cc0b44",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download started...\n",
      "dataset ready\n",
      "FASTA files:\n",
      "dataset/ncbi_dataset/data/GCF_000001405.40/GCF_000001405.40_GRCh38.p14_genomic.fna\n"
     ]
    }
   ],
   "source": [
    "# datasets\n",
    "hg38_url = 'https://api.ncbi.nlm.nih.gov/datasets/v2alpha/genome/accession/GCF_000001405.40/download'\n",
    "t2t_url = 'https://api.ncbi.nlm.nih.gov/datasets/v2alpha/genome/accession/GCF_009914755.1/download'\n",
    "dataset_url = hg38\n",
    "\n",
    "print(\"download started...\")\n",
    "response = requests.get(dataset_url, params={'include_annotation_type': 'GENOME_FASTA'})\n",
    "if response.status_code == 200:\n",
    "    data_dir_path = 'dataset'\n",
    "    os.makedirs(data_dir_path, exist_ok=True)\n",
    "    with BytesIO(response.content) as zip_buffer:\n",
    "        ZipFile(zip_buffer, 'r').extractall(path=data_dir_path)\n",
    "    print(\"dataset ready\")\n",
    "\n",
    "gh38_fasta = 'dataset/ncbi_dataset/data/GCF_000001405.40/GCF_000001405.40_GRCh38.p14_genomic.fna'\n",
    "\n",
    "print(\"FASTA files:\")\n",
    "fpaths = list(Path('dataset').rglob('*.fna'))\n",
    "for fpath in fpaths:\n",
    "    print(fpath)\n",
    "\n",
    "data_path = fpaths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac19307-d524-4eb1-af22-75ad8eed139c",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0071b7-7054-47db-bb69-00ee4d964980",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aff9a9d-df9c-42cc-b946-58cb4de6b1c0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecb0435f-d1a0-44a2-ace1-b29cfb926da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "GPU count: 8\n",
      "Current GPU: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "gpu = 4\n",
    "torch.cuda.set_device(gpu)\n",
    "print(\"Current GPU:\", torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348144e-b691-4ea6-8c0f-64c161124c78",
   "metadata": {},
   "source": [
    "## Need to check model\n",
    "- Allocate inference cache? E.g., [here](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L142)\n",
    "- Init weights\n",
    "- **PARAMETERS**\n",
    "- Loss function? CrossEntropyLoss?\n",
    "- Optimizer? Adam?\n",
    "    - Note: HyenaDNA used AdamW\n",
    "### MambaTower\n",
    "- ~~no embedding like [here](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L149)? -> In MambaDNA~~\n",
    "- ~~put nn.Linear behind? As in [MambaLMHeadModel](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L224) -> In MambaDNA~~\n",
    "### MambaBlock\n",
    "- [residual?](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L152)?\n",
    "- Different order: Original: Add -> LN -> MAMBA ([see](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L334-L350)); Here: MAMBA -> Add -> LN\n",
    "    - Does this remove the need for residual? Yes\n",
    "    - Is this equivalent?\n",
    "        - Close: MAMBA block is: `x = self.mamba(self.norm(x)) + x`\n",
    "        - Does it matter if residual connection does not include LN?\n",
    "    - Fuzed normalization (with add) used for higher performance ([see](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L311))\n",
    "    - Put LN in front (`x = self.mamba(self.norm(x)) + x`); would need to add normalization behind as well\n",
    "- Normalization: LayerNorm or RMSNorm?\n",
    "- Fuse normalization with add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0482a8b-d3f6-4b07-825d-a27c6f41870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://github.com/apapiu/mamba_small_bench\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout_level=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mamba = Mamba(d_model=embed_dim, d_state=16, d_conv=4, expand=2)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_level)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(self.mamba(x) + x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MambaTower(nn.Module):\n",
    "    def __init__(self, embed_dim, n_layers, seq_len=None, global_pool=False):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(*[MambaBlock(embed_dim) for _ in range(n_layers)])\n",
    "        self.global_pool = global_pool #for classification or other supervised learning.\n",
    "\n",
    "    def forward(self, x):\n",
    "        #for input (bs, n, d) it returns either (bs, n, d) or (bs, d) is global_pool\n",
    "        out = self.blocks(x) if not self.global_pool else torch.mean(self.blocks(x),1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MambaDNA(nn.Module):\n",
    "    def __init__(self, embed_dim, seq_len, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.tower = MambaTower(embed_dim, n_layers, seq_len=seq_len, global_pool=False)\n",
    "        self.out_proj = nn.Sequential(nn.LayerNorm(embed_dim),\n",
    "                                      nn.Linear(embed_dim, vocab_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tower(self.embed(x))\n",
    "        return self.out_proj(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
