{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98235891-ea23-416a-b32d-58467e59d458",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Dataloader (pretraining)\n",
    "* [HyenaDNA HG38 dataloader](https://github.com/HazyResearch/hyena-dna/blob/main/src/dataloaders/datasets/hg38_dataset.py)\n",
    "* HyenaDNA used training/validation intervals from *Effective gene expression prediction from sequence by integrating long-range interactions.* paper.\n",
    "\n",
    "## Tokenizer?\n",
    "* Need to check HyenaDNA; I think their Jupyter contained some code of their tokenizer\n",
    "\n",
    "## Model\n",
    "* [Original MAMBA repo](https://github.com/state-spaces/mamba)\n",
    "    * [benchmark_generation_mamba_simple.py](https://github.com/state-spaces/mamba/blob/main/benchmarks/benchmark_generation_mamba_simple.py)\n",
    "    * Uses [mambaLMHeadModel](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L173) form `mixer_seq_simple.py`\n",
    "    * Uses [MixerModel](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L83)\n",
    "    * Uses [create_block](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L21)\n",
    "    * Uses [Block](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L298) and [MAMBA](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L34) classes (from `mamba_simple.py`)\n",
    "        * Actual MAMBA operation: [mamba_inner_fn](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/ops/selective_scan_interface.py#L155)\n",
    "* [Mamba small benchmark repo](https://github.com/apapiu/mamba_small_bench)\n",
    "* [SimplerMambaSSM Jupyter Notebook](./SimplerMambaSSM.ipynb)\n",
    "    * Use mamba-ssm library\n",
    "    * See class BigNeuralNetwork\n",
    "* [MAMBA chat](https://github.com/havenhq/mamba-chat/blob/main/train_mamba.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c3297-f67c-4ac8-8403-f98a60637cc9",
   "metadata": {},
   "source": [
    "# Required python packages\n",
    "1. PyTorch (with CUDA)\n",
    "2. mamba-ssm\n",
    "3. transformers==4.26.1 *(for tokenizer)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75af3307-1af8-4faf-b32b-99de0382706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "from pyfaidx import Fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec1144b-3e2a-4cbd-9a73-db90afc31f48",
   "metadata": {},
   "source": [
    "# Download genetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09f33b3a-1046-4a84-9d74-4cf1d5cc0b44",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download started...\n",
      "dataset ready\n",
      "FASTA files:\n",
      "dataset/ncbi_dataset/data/GCF_009914755.1/GCF_009914755.1_T2T-CHM13v2.0_genomic.fna\n"
     ]
    }
   ],
   "source": [
    "# datasets\n",
    "hg38_url = 'https://api.ncbi.nlm.nih.gov/datasets/v2alpha/genome/accession/GCF_000001405.40/download'\n",
    "t2t_url = 'https://api.ncbi.nlm.nih.gov/datasets/v2alpha/genome/accession/GCF_009914755.1/download'\n",
    "dataset_url = t2t_url\n",
    "\n",
    "print(\"download started...\")\n",
    "response = requests.get(dataset_url, params={'include_annotation_type': 'GENOME_FASTA'})\n",
    "if response.status_code == 200:\n",
    "    data_dir_path = 'dataset'\n",
    "    os.makedirs(data_dir_path, exist_ok=True)\n",
    "    with BytesIO(response.content) as zip_buffer:\n",
    "        ZipFile(zip_buffer, 'r').extractall(path=data_dir_path)\n",
    "    print(\"dataset ready\")\n",
    "\n",
    "gh38_fasta = 'dataset/ncbi_dataset/data/GCF_000001405.40/GCF_000001405.40_GRCh38.p14_genomic.fna'\n",
    "\n",
    "print(\"FASTA files:\")\n",
    "fpaths = list(Path('dataset').rglob('*.fna'))\n",
    "for fpath in fpaths:\n",
    "    print(fpath)\n",
    "\n",
    "fasta_path = fpaths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac19307-d524-4eb1-af22-75ad8eed139c",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9f0071b7-7054-47db-bb69-00ee4d964980",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HyenaDNA tokenizer; code from their jupyter notebook\n",
    "\"\"\"\n",
    "Just a simple character level tokenizer.\n",
    "\n",
    "From: https://github.com/dariush-bahrami/character-tokenizer/blob/master/charactertokenizer/core.py\n",
    "\n",
    "CharacterTokenzier for Hugging Face Transformers.\n",
    "This is heavily inspired from CanineTokenizer in transformers package.\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class CharacterTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token called\n",
    "                [UNK] with id=6. Following are list of all of the special tokens with\n",
    "                their corresponding ids:\n",
    "                    \"[CLS]\": 0\n",
    "                    \"[SEP]\": 1\n",
    "                    \"[BOS]\": 2\n",
    "                    \"[MASK]\": 3\n",
    "                    \"[PAD]\": 4\n",
    "                    \"[RESERVED]\": 5\n",
    "                    \"[UNK]\": 6\n",
    "                an id (starting at 7) will be assigned to each character.\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "\n",
    "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=sep_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_side=padding_side,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def convert_token_vector_to_string(self, ivector):\n",
    "        out_str = \"\"\n",
    "        for i in ivector:\n",
    "            out_str = out_str + self._convert_id_to_token(i.item())\n",
    "        return out_str\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        result = cls + token_ids_0 + sep\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "        return result\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf372c-23c9-4f40-b25d-9c617abeb126",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f3d2ec6e-8035-4d95-a49a-18753c25c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement(in_seq):\n",
    "    out_seq = \"\"\n",
    "    for idx, c in enumerate(in_seq):\n",
    "        oc = \"X\"\n",
    "        if c == 'A':\n",
    "            oc = 'T'\n",
    "        elif c == 'T':\n",
    "            oc = 'A'\n",
    "        elif c == 'C':\n",
    "            oc = 'G'\n",
    "        elif c == 'G':\n",
    "            oc = 'C'\n",
    "        elif c == 'N':\n",
    "            oc = 'N'\n",
    "        else:\n",
    "            assert True == False\n",
    "        out_seq = out_seq + oc\n",
    "    return out_seq\n",
    "    \n",
    "\n",
    "class GenomeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, fasta_path, tokenizer, seq_len, split='train'):\n",
    "        assert split == 'train' or split == 'test'\n",
    "        \n",
    "        assert Path(fasta_path).exists\n",
    "        self.fasta = Fasta(fasta_path, one_based_attributes=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        keys = self.fasta.keys()\n",
    "\n",
    "        dtype = np.dtype([('key', 'U20'), ('start', 'int_'), ('end', 'int_')])\n",
    "        entry_ranges = np.array([], dtype=dtype)\n",
    "\n",
    "        count = 0\n",
    "        for idx, k in enumerate(keys):\n",
    "            # split in training and test dataset\n",
    "            if (((idx - 5) % 10 != 0) == (split == \"train\")):\n",
    "                seq_len = len(self.fasta[k])\n",
    "                e = np.array([(k, count, count + seq_len)], dtype=dtype)\n",
    "                entry_ranges = np.append(entry_ranges, e)\n",
    "                count = count + len(self.fasta[k])\n",
    "            \n",
    "        self.entry_ranges = entry_ranges.copy(order='C')\n",
    "\n",
    "        for e in self.entry_ranges:\n",
    "            print(e)\n",
    "             \n",
    "    def __len__(self):\n",
    "        # first range forward idices, second range reverse complement\n",
    "        return self.entry_ranges[-1]['end'] * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx >= 0\n",
    "        assert idx < self.__len__()\n",
    "\n",
    "        # return reverse complement?\n",
    "        rev_compl = (idx >= self.entry_ranges[-1]['end'])\n",
    "        idx = idx % self.entry_ranges[-1]['end']\n",
    "\n",
    "        # locate FASTA entry of global idx\n",
    "        key = None\n",
    "        local_idx = -1\n",
    "        for e in self.entry_ranges:\n",
    "            if e['start'] <= idx < e['end']:\n",
    "                key = e['key']\n",
    "                local_idx = idx - e['start']\n",
    "\n",
    "        assert key != None\n",
    "        assert local_idx != -1\n",
    "        print(\"local_idx: {}\".format(local_idx))\n",
    "\n",
    "        left_bound = 0\n",
    "        right_bound = len(self.fasta[key])\n",
    "\n",
    "        print(self.fasta[key][-20:], len(self.fasta[key][-20:]))\n",
    "\n",
    "        seq = None\n",
    "        if not(rev_compl):\n",
    "            seq = self.fasta[key][:local_idx + 1][-self.seq_len:]\n",
    "        else:\n",
    "            seq = self.fasta[key][local_idx:][::-1][-self.seq_len:]\n",
    "        assert seq != None\n",
    "\n",
    "        # capitalize all nucleotides\n",
    "        seq_str = str(seq).upper()\n",
    "        # print(\"seq_str: {}\".format(seq_str))\n",
    "\n",
    "        # use complement when reverse\n",
    "        if rev_compl:\n",
    "            seq_str = complement(seq_str)\n",
    "        print(seq_str, len(seq_str))\n",
    "        assert len(seq_str) <= self.seq_len\n",
    "\n",
    "        tokens = self.tokenizer(seq_str, add_special_tokens=False, padding=\"max_length\",\n",
    "                                max_length=self.seq_len, truncation=True)\n",
    "        input = torch.LongTensor(tokens[\"input_ids\"]).clone()\n",
    "        # print(\"input: {}\".format(input))\n",
    "\n",
    "        # mask\n",
    "        target = input[-1].clone()\n",
    "        input[-1] = self.tokenizer._vocab_str_to_int['[MASK]']\n",
    "        print(input, target)\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5e201da3-ea6f-404d-b862-a62f28508f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NC_060925.1', 0, 248387328)\n",
      "('NC_060926.1', 248387328, 491084080)\n",
      "('NC_060927.1', 491084080, 692190028)\n",
      "('NC_060928.1', 692190028, 885764973)\n",
      "('NC_060929.1', 885764973, 1067810412)\n",
      "('NC_060931.1', 1067810412, 1228377840)\n",
      "('NC_060932.1', 1228377840, 1374637171)\n",
      "('NC_060933.1', 1374637171, 1525254418)\n",
      "('NC_060934.1', 1525254418, 1660012552)\n",
      "('NC_060935.1', 1660012552, 1795140321)\n",
      "('NC_060936.1', 1795140321, 1928464869)\n",
      "('NC_060937.1', 1928464869, 2042031555)\n",
      "('NC_060938.1', 2042031555, 2143193047)\n",
      "('NC_060939.1', 2143193047, 2242946242)\n",
      "('NC_060941.1', 2242946242, 2327223139)\n",
      "('NC_060942.1', 2327223139, 2407765677)\n",
      "('NC_060943.1', 2407765677, 2469473041)\n",
      "('NC_060944.1', 2469473041, 2535683296)\n",
      "('NC_060945.1', 2535683296, 2580773978)\n",
      "('NC_060946.1', 2580773978, 2632098904)\n",
      "('NC_060947.1', 2632098904, 2786358470)\n",
      "('NC_060948.1', 2786358470, 2848818499)\n",
      "local_idx: 5\n",
      "ttagggttagggttagggtt 20\n",
      "CACCCT 6\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        8, 7, 8, 8, 8, 3]) tensor(10)\n",
      "actual_inpt: [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]CACCC[MASK]\n",
      "actual_trgt: T\n",
      "tests completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_dataset_class():\n",
    "    max_length = 30\n",
    "    tokenizer = CharacterTokenizer(\n",
    "        characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
    "        model_max_length=max_length,  # to account for special tokens, like EOS\n",
    "        add_special_tokens=False,  # we handle special tokens elsewhere\n",
    "        padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
    "    )\n",
    "    dataset = GenomeDataset(fasta_path, tokenizer, max_length, split=\"train\")\n",
    "    \n",
    "    def check(idx, expct_inpt, expct_trgt):\n",
    "        inpt, trgt = dataset.__getitem__(5)\n",
    "        actual_trgt = tokenizer._convert_id_to_token(trgt.item())\n",
    "        actual_inpt = tokenizer.convert_token_vector_to_string(inpt)\n",
    "        # print(\"expct_inpt: {}\".format(expct_inpt))\n",
    "        print(\"actual_inpt: {}\".format(actual_inpt))\n",
    "        print(\"actual_trgt: {}\".format(actual_trgt))\n",
    "        assert expct_trgt == actual_trgt\n",
    "        assert expct_inpt == actual_inpt\n",
    "\n",
    "    check(5, \"[PAD]\"*24 + \"CACCC\" + \"[MASK]\", \"T\")\n",
    "\n",
    "    print(\"tests completed successfully!\")\n",
    "test_dataset_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4f20da3f-e529-4db9-9ac5-980024002085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_idx: 5\n",
      "ttagggttagggttagggtt 20\n",
      "CACCCT 6\n",
      "tensor([4, 4, 4, 4, 8, 7, 8, 8, 8, 3]) tensor(10)\n",
      "local_idx: 5\n",
      "ttagggttagggttagggtt 20\n",
      "TTAGGGTTTA 10\n",
      "tensor([10, 10,  7,  9,  9,  9, 10, 10, 10,  3]) tensor(7)\n",
      "local_idx: 248387323\n",
      "ttagggttagggttagggtt 20\n",
      "TTAGGGTTAG 10\n",
      "tensor([10, 10,  7,  9,  9,  9, 10, 10,  7,  3]) tensor(9)\n",
      "local_idx: 248387323\n",
      "ttagggttagggttagggtt 20\n",
      "AACCC 5\n",
      "tensor([4, 4, 4, 4, 4, 7, 7, 8, 8, 3]) tensor(8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([4, 4, 4, 4, 4, 7, 7, 8, 8, 3]), tensor(8))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = CharacterTokenizer(\n",
    "    characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
    "    model_max_length=max_length + 2,  # to account for special tokens, like EOS\n",
    "    add_special_tokens=False,  # we handle special tokens elsewhere\n",
    "    padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
    ")\n",
    "\n",
    "dataset = GenomeDataset(fasta_path, tokenizer, 10, split=\"train\")\n",
    "dataset.__getitem__(5)\n",
    "dataset.__getitem__(dataset.__len__()//2 + 5)\n",
    "dataset.__getitem__(248387317)\n",
    "dataset.__getitem__(dataset.__len__()//2 + 248387317)\n",
    "# dataset.__getitem__(248387338)\n",
    "# dataset.__getitem__(2786358460)\n",
    "# dataset.__getitem__(1928464869)\n",
    "\n",
    "# tokenizer(\"ATGAAGGAAGG\", \n",
    "#                 add_special_tokens=False, \n",
    "#                 padding=\"max_length\",\n",
    "#                 max_length=10,\n",
    "#                 truncation=True,\n",
    "#             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff9a9d-df9c-42cc-b946-58cb4de6b1c0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecb0435f-d1a0-44a2-ace1-b29cfb926da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "GPU count: 8\n",
      "Current GPU: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "gpu = 4\n",
    "torch.cuda.set_device(gpu)\n",
    "print(\"Current GPU:\", torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348144e-b691-4ea6-8c0f-64c161124c78",
   "metadata": {},
   "source": [
    "## Investigations\n",
    "- Allocate inference cache? E.g., [here](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L142)\n",
    "- Init weights\n",
    "- **PARAMETERS**\n",
    "- Loss function? CrossEntropyLoss?\n",
    "- Optimizer? Adam?\n",
    "    - Note: HyenaDNA used AdamW\n",
    "### MambaTower\n",
    "- ~~no embedding like [here](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L149)? -> In MambaDNA~~\n",
    "- Do I need an embedding, for my small set of tokens?\n",
    "- ~~put nn.Linear behind? As in [MambaLMHeadModel](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L224) -> In MambaDNA~~\n",
    "### MambaBlock\n",
    "- [residual?](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L152)?\n",
    "- Different order: Original: Add -> LN -> MAMBA ([see](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L334-L350)); Here: MAMBA -> Add -> LN\n",
    "    - Does this remove the need for residual? Yes\n",
    "    - Is this equivalent?\n",
    "        - Close: MAMBA block is: `x = self.mamba(self.norm(x)) + x`\n",
    "        - Does it matter if residual connection does not include LN?\n",
    "    - Fuzed normalization (with add) used for higher performance ([see](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L311))\n",
    "    - Put LN in front (`x = self.mamba(self.norm(x)) + x`); would need to add normalization behind as well\n",
    "- Normalization: LayerNorm or RMSNorm?\n",
    "- Fuse normalization with add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0482a8b-d3f6-4b07-825d-a27c6f41870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://github.com/apapiu/mamba_small_bench\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout_level=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mamba = Mamba(d_model=embed_dim, d_state=16, d_conv=4, expand=2)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_level)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(self.mamba(x) + x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MambaTower(nn.Module):\n",
    "    def __init__(self, embed_dim, n_layers, seq_len=None, global_pool=False):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(*[MambaBlock(embed_dim) for _ in range(n_layers)])\n",
    "        self.global_pool = global_pool #for classification or other supervised learning.\n",
    "\n",
    "    def forward(self, x):\n",
    "        #for input (bs, n, d) it returns either (bs, n, d) or (bs, d) is global_pool\n",
    "        out = self.blocks(x) if not self.global_pool else torch.mean(self.blocks(x),1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MambaDNA(nn.Module):\n",
    "    def __init__(self, embed_dim, seq_len, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.tower = MambaTower(embed_dim, n_layers, seq_len=seq_len, global_pool=False)\n",
    "        self.out_proj = nn.Sequential(nn.LayerNorm(embed_dim),\n",
    "                                      nn.Linear(embed_dim, vocab_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tower(self.embed(x))\n",
    "        return self.out_proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46aff14-1adb-4841-8f69-19302c34d5db",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cb92b20-98dc-46f1-a8b2-51258a58ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1000\n",
    "\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = CharacterTokenizer(\n",
    "    characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
    "    model_max_length=max_length + 2,  # to account for special tokens, like EOS\n",
    "    add_special_tokens=False,  # we handle special tokens elsewhere\n",
    "    padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
