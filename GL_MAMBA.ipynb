{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98235891-ea23-416a-b32d-58467e59d458",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Dataloader (pretraining)\n",
    "* [HyenaDNA HG38 dataloader](https://github.com/HazyResearch/hyena-dna/blob/main/src/dataloaders/datasets/hg38_dataset.py)\n",
    "* HyenaDNA used training/validation intervals from *Effective gene expression prediction from sequence by integrating long-range interactions.* paper.\n",
    "\n",
    "## Tokenizer?\n",
    "* Need to check HyenaDNA; I think their Jupyter contained some code of their tokenizer\n",
    "\n",
    "## Model\n",
    "* [Original MAMBA repo](https://github.com/state-spaces/mamba)\n",
    "    * [benchmark_generation_mamba_simple.py](https://github.com/state-spaces/mamba/blob/main/benchmarks/benchmark_generation_mamba_simple.py)\n",
    "    * Uses [mambaLMHeadModel](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L173) form `mixer_seq_simple.py`\n",
    "    * Uses [MixerModel](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L83)\n",
    "    * Uses [create_block](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L21)\n",
    "    * Uses [Block](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L298) and [MAMBA](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L34) classes (from `mamba_simple.py`)\n",
    "        * Actual MAMBA operation: [mamba_inner_fn](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/ops/selective_scan_interface.py#L155)\n",
    "* [Mamba small benchmark repo](https://github.com/apapiu/mamba_small_bench)\n",
    "* [SimplerMambaSSM Jupyter Notebook](./SimplerMambaSSM.ipynb)\n",
    "    * Use mamba-ssm library\n",
    "    * See class BigNeuralNetwork\n",
    "* [MAMBA chat](https://github.com/havenhq/mamba-chat/blob/main/train_mamba.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c3297-f67c-4ac8-8403-f98a60637cc9",
   "metadata": {},
   "source": [
    "# Required python packages\n",
    "1. PyTorch (with CUDA)\n",
    "2. mamba-ssm\n",
    "3. transformers==4.26.1 *(for tokenizer)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75af3307-1af8-4faf-b32b-99de0382706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "from pyfaidx import Fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec1144b-3e2a-4cbd-9a73-db90afc31f48",
   "metadata": {},
   "source": [
    "# Download genetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09f33b3a-1046-4a84-9d74-4cf1d5cc0b44",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download started...\n",
      "dataset ready\n",
      "FASTA files:\n",
      "dataset/ncbi_dataset/data/GCF_009914755.1/GCF_009914755.1_T2T-CHM13v2.0_genomic.fna\n"
     ]
    }
   ],
   "source": [
    "# datasets\n",
    "hg38_url = 'https://api.ncbi.nlm.nih.gov/datasets/v2alpha/genome/accession/GCF_000001405.40/download'\n",
    "t2t_url = 'https://api.ncbi.nlm.nih.gov/datasets/v2alpha/genome/accession/GCF_009914755.1/download'\n",
    "dataset_url = t2t_url\n",
    "\n",
    "print(\"download started...\")\n",
    "response = requests.get(dataset_url, params={'include_annotation_type': 'GENOME_FASTA'})\n",
    "if response.status_code == 200:\n",
    "    data_dir_path = 'dataset'\n",
    "    os.makedirs(data_dir_path, exist_ok=True)\n",
    "    with BytesIO(response.content) as zip_buffer:\n",
    "        ZipFile(zip_buffer, 'r').extractall(path=data_dir_path)\n",
    "    print(\"dataset ready\")\n",
    "\n",
    "gh38_fasta = 'dataset/ncbi_dataset/data/GCF_000001405.40/GCF_000001405.40_GRCh38.p14_genomic.fna'\n",
    "\n",
    "print(\"FASTA files:\")\n",
    "fpaths = list(Path('dataset').rglob('*.fna'))\n",
    "for fpath in fpaths:\n",
    "    print(fpath)\n",
    "\n",
    "fasta_path = fpaths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac19307-d524-4eb1-af22-75ad8eed139c",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9f0071b7-7054-47db-bb69-00ee4d964980",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HyenaDNA tokenizer; code from their jupyter notebook\n",
    "\"\"\"\n",
    "Just a simple character level tokenizer.\n",
    "\n",
    "From: https://github.com/dariush-bahrami/character-tokenizer/blob/master/charactertokenizer/core.py\n",
    "\n",
    "CharacterTokenzier for Hugging Face Transformers.\n",
    "This is heavily inspired from CanineTokenizer in transformers package.\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class CharacterTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token called\n",
    "                [UNK] with id=6. Following are list of all of the special tokens with\n",
    "                their corresponding ids:\n",
    "                    \"[CLS]\": 0\n",
    "                    \"[SEP]\": 1\n",
    "                    \"[BOS]\": 2\n",
    "                    \"[MASK]\": 3\n",
    "                    \"[PAD]\": 4\n",
    "                    \"[RESERVED]\": 5\n",
    "                    \"[UNK]\": 6\n",
    "                an id (starting at 7) will be assigned to each character.\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "\n",
    "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=sep_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_side=padding_side,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def convert_token_vector_to_string(self, ivector):\n",
    "        out_str = \"\"\n",
    "        for i in ivector:\n",
    "            out_str = out_str + self._convert_id_to_token(i.item())\n",
    "        return out_str\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        result = cls + token_ids_0 + sep\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "        return result\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf372c-23c9-4f40-b25d-9c617abeb126",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f3d2ec6e-8035-4d95-a49a-18753c25c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement(in_seq):\n",
    "    out_seq = \"\"\n",
    "    for idx, c in enumerate(in_seq):\n",
    "        oc = \"X\"\n",
    "        if c == 'A':\n",
    "            oc = 'T'\n",
    "        elif c == 'T':\n",
    "            oc = 'A'\n",
    "        elif c == 'C':\n",
    "            oc = 'G'\n",
    "        elif c == 'G':\n",
    "            oc = 'C'\n",
    "        elif c == 'N':\n",
    "            oc = 'N'\n",
    "        else:\n",
    "            assert True == False\n",
    "        out_seq = out_seq + oc\n",
    "    return out_seq\n",
    "    \n",
    "\n",
    "class GenomeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, fasta_path, ds_entries):\n",
    "        assert Path(fasta_path).exists\n",
    "        self.fasta = Fasta(fasta_path, one_based_attributes=False)\n",
    "\n",
    "        dtype = np.dtype([('key', 'U20'), ('start', 'int_'), ('end', 'int_')])\n",
    "        self.entry_ranges = np.empty(len(ds_entries), dtype=dtype)\n",
    "\n",
    "        # only append entries of dataset\n",
    "        count = 0\n",
    "        for idx, k in enumerate(ds_entries):\n",
    "            assert k in self.fasta.keys(), \\\n",
    "                \"FASTA file does not contain an entry with key {}\".format(k)\n",
    "            seq_len = len(self.fasta[k])\n",
    "            self.entry_ranges[idx] = np.array([(k, count, count + seq_len)], dtype=dtype)\n",
    "            count = count + seq_len\n",
    "\n",
    "        # for e in self.entry_ranges:\n",
    "        #     print(e)\n",
    "\n",
    "    def config(self, tokenizer, seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "             \n",
    "    def __len__(self):\n",
    "        # first range forward idices, second range reverse complement\n",
    "        return self.entry_ranges[-1]['end'] * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx >= 0\n",
    "        assert idx < self.__len__()\n",
    "\n",
    "        # return reverse complement?\n",
    "        rev_compl = (idx >= self.entry_ranges[-1]['end'])\n",
    "        idx = idx % self.entry_ranges[-1]['end']\n",
    "\n",
    "        # locate FASTA entry of global idx\n",
    "        key = None\n",
    "        local_idx = -1\n",
    "        for e in self.entry_ranges:\n",
    "            if e['start'] <= idx < e['end']:\n",
    "                key = e['key']\n",
    "                local_idx = idx - e['start']\n",
    "\n",
    "        assert key != None\n",
    "        assert local_idx != -1\n",
    "        # print(\"local_idx: {}, rev_compl: {}\".format(local_idx, rev_compl))\n",
    "\n",
    "        left_bound = 0\n",
    "        right_bound = len(self.fasta[key])\n",
    "\n",
    "        # print(self.fasta[key][-20:], len(self.fasta[key][-20:]))\n",
    "\n",
    "        seq = None\n",
    "        if not(rev_compl):\n",
    "            seq = self.fasta[key][:local_idx + 1][-self.seq_len:]\n",
    "        else:\n",
    "            seq = self.fasta[key][local_idx:][::-1][-self.seq_len:]\n",
    "        assert seq != None\n",
    "\n",
    "        # capitalize all nucleotides\n",
    "        seq_str = str(seq).upper()\n",
    "        # print(\"seq_str: {}\".format(seq_str))\n",
    "\n",
    "        # use complement when reverse\n",
    "        if rev_compl:\n",
    "            seq_str = complement(seq_str)\n",
    "        # print(seq_str, len(seq_str))\n",
    "        assert len(seq_str) <= self.seq_len\n",
    "\n",
    "        tokens = self.tokenizer(seq_str, add_special_tokens=False, padding=\"max_length\",\n",
    "                                max_length=self.seq_len, truncation=True)\n",
    "        input = torch.LongTensor(tokens[\"input_ids\"]).clone()\n",
    "        # print(\"input: {}\".format(input))\n",
    "\n",
    "        # mask\n",
    "        target = input[-1].clone()\n",
    "        input[-1] = self.tokenizer._vocab_str_to_int['[MASK]']\n",
    "        # print(input, target)\n",
    "        return input, target\n",
    "\n",
    "def get_T2T_datasets():\n",
    "    T2T_path = \"dataset/ncbi_dataset/data/GCF_009914755.1/GCF_009914755.1_T2T-CHM13v2.0_genomic.fna\"\n",
    "    training_entries = ['NC_060925.1', 'NC_060926.1', 'NC_060927.1', 'NC_060928.1', 'NC_060929.1',\n",
    "                        'NC_060931.1', 'NC_060932.1', 'NC_060933.1', 'NC_060934.1', 'NC_060935.1',\n",
    "                        'NC_060936.1', 'NC_060937.1', 'NC_060938.1', 'NC_060939.1', 'NC_060941.1',\n",
    "                        'NC_060942.1', 'NC_060943.1', 'NC_060944.1', 'NC_060945.1', 'NC_060946.1',\n",
    "                        'NC_060947.1', 'NC_060948.1']\n",
    "    test_entries = ['NC_060930.1', 'NC_060940.1']\n",
    "\n",
    "    # check training and test dataset do not contain the same entries\n",
    "    assert set(training_entries).isdisjoint(set(test_entries)) == True\n",
    "    \n",
    "    train_dataset = GenomeDataset(T2T_path, training_entries)\n",
    "    test_dataset = GenomeDataset(T2T_path, test_entries)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# train_ds, test_ds = get_T2T_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "5e201da3-ea6f-404d-b862-a62f28508f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NC_060925.1',          0,  248387328)\n",
      " ('NC_060926.1',  248387328,  491084080)\n",
      " ('NC_060927.1',  491084080,  692190028)\n",
      " ('NC_060928.1',  692190028,  885764973)\n",
      " ('NC_060929.1',  885764973, 1067810412)\n",
      " ('NC_060931.1', 1067810412, 1228377840)\n",
      " ('NC_060932.1', 1228377840, 1374637171)\n",
      " ('NC_060933.1', 1374637171, 1525254418)\n",
      " ('NC_060934.1', 1525254418, 1660012552)\n",
      " ('NC_060935.1', 1660012552, 1795140321)\n",
      " ('NC_060936.1', 1795140321, 1928464869)\n",
      " ('NC_060937.1', 1928464869, 2042031555)\n",
      " ('NC_060938.1', 2042031555, 2143193047)\n",
      " ('NC_060939.1', 2143193047, 2242946242)\n",
      " ('NC_060941.1', 2242946242, 2327223139)\n",
      " ('NC_060942.1', 2327223139, 2407765677)\n",
      " ('NC_060943.1', 2407765677, 2469473041)\n",
      " ('NC_060944.1', 2469473041, 2535683296)\n",
      " ('NC_060945.1', 2535683296, 2580773978)\n",
      " ('NC_060946.1', 2580773978, 2632098904)\n",
      " ('NC_060947.1', 2632098904, 2786358470)\n",
      " ('NC_060948.1', 2786358470, 2848818499)]\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Input tokens do not match; expected: [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]CTAACCCTAACCCTAACCCT[MASK] (len 76), actual: [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]CTAACCCTAACCCTAACCCT[MASK] (len 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[293], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     check(\u001b[38;5;241m2848818478\u001b[39m \u001b[38;5;241m+\u001b[39m offset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCTAACCCTAACCCTAACCCT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtests completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m \u001b[43mvalidate_T2T_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[293], line 49\u001b[0m, in \u001b[0;36mvalidate_T2T_ds\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m check(\u001b[38;5;241m2786358510\u001b[39m \u001b[38;5;241m+\u001b[39m offset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGTTAGGGTTAGGGTTAGGGTTAGGGTTAG\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGATTGGGATTGGGATTGGGA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2848818478\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[PAD]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCTAACCCTAACCCTAACCCT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[MASK]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtests completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[293], line 30\u001b[0m, in \u001b[0;36mvalidate_T2T_ds.<locals>.check\u001b[0;34m(idx, expct_inpt, expct_trgt)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Does not recognize \"PAD\" and \"MASK\" token\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# expct_inpt_len = len(tokenizer(expct_inpt, add_special_tokens=False, max_length=token_len,\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#                            truncation=False)[\"input_ids\"])\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# assert expct_inpt_len == token_len, \\\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     \"Unexpected token length of expct_inpt ({})\".format(expct_inpt_len)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m expct_trgt \u001b[38;5;241m==\u001b[39m actual_trgt, \\\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget tokens do not match; expected: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, actual: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(expct_trgt, actual_trgt)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m expct_inpt \u001b[38;5;241m==\u001b[39m actual_inpt, \\\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput tokens do not match; expected: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (len \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m), actual: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (len \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(expct_inpt, \u001b[38;5;28mlen\u001b[39m(expct_inpt), actual_inpt, \u001b[38;5;28mlen\u001b[39m(actual_inpt))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens match!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Input tokens do not match; expected: [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]CTAACCCTAACCCTAACCCT[MASK] (len 76), actual: [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]CTAACCCTAACCCTAACCCT[MASK] (len 71)"
     ]
    }
   ],
   "source": [
    "# validate results of T2T dataset;\n",
    "# check if correct tokens are returned for predefined indices\n",
    "def validate_T2T_ds():\n",
    "    train_ds, _ = get_T2T_datasets()\n",
    "    \n",
    "    token_len = 30\n",
    "    tokenizer = CharacterTokenizer(\n",
    "        characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
    "        model_max_length=token_len,  # to account for special tokens, like EOS\n",
    "        add_special_tokens=False,  # we handle special tokens elsewhere\n",
    "        padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
    "    )\n",
    "\n",
    "    train_ds.config(tokenizer, token_len)\n",
    "    print(train_ds.entry_ranges)\n",
    "    \n",
    "    def check(idx, expct_inpt, expct_trgt):\n",
    "        inpt, trgt = train_ds.__getitem__(idx)\n",
    "        actual_trgt = tokenizer._convert_id_to_token(trgt.item())\n",
    "        actual_inpt = tokenizer.convert_token_vector_to_string(inpt)\n",
    "\n",
    "        # Does not recognize \"PAD\" and \"MASK\" token\n",
    "        # expct_inpt_len = len(tokenizer(expct_inpt, add_special_tokens=False, max_length=token_len,\n",
    "        #                            truncation=False)[\"input_ids\"])\n",
    "        # assert expct_inpt_len == token_len, \\\n",
    "        #     \"Unexpected token length of expct_inpt ({})\".format(expct_inpt_len)\n",
    "        \n",
    "        assert expct_trgt == actual_trgt, \\\n",
    "            \"Target tokens do not match; expected: {}, actual: {}\".format(expct_trgt, actual_trgt)\n",
    "        assert expct_inpt == actual_inpt, \\\n",
    "            \"Input tokens do not match; expected: {} (len {}), actual: {} (len {})\".format(expct_inpt, len(expct_inpt), actual_inpt, len(actual_inpt))\n",
    "        print(\"tokens match!\")\n",
    "\n",
    "    # forward\n",
    "    check(5, \"[PAD]\"*24 + \"CACCC\" + \"[MASK]\", \"T\")\n",
    "    check(248387322, \"GGGTTAGGGTTAGGGTTAGGGTTAGGGTT\" + \"[MASK]\", \"A\")\n",
    "    check(1067810436, \"[PAD]\"*5 + \"CCTAACCCTAACCCTAACCCCTAA\" + \"[MASK]\", \"C\")\n",
    "    check(1228377838, \"GGGTTAGGGTTAGGGGTTAGGGTTAGGGT\" + \"[MASK]\", \"T\")\n",
    "    check(2786358510, \"CTAACCCTAACCCTAACCCTAACCCTAAC\" + \"[MASK]\", \"C\")\n",
    "    check(2848818478, \"AGGGTTAGGGTTAGGGTTAGGGTTAGGGT\" + \"[MASK]\", \"T\")\n",
    "    # reverse complement\n",
    "    offset = 2848818499\n",
    "    check((5 + offset), \"GTTAGGGTTAGGGTTAGGGGTTAGGGTTT\" + \"[MASK]\", \"A\")\n",
    "    check(248387322 + offset, \"[PAD]\"*24 + \"AACCC\" + \"[MASK]\", \"T\")\n",
    "    check(1067810436 + offset, \"GTTAGGAGGGTTAGGGGATTAGGGTTAGG\" + \"[MASK]\", \"G\")\n",
    "    check(1228377838 + offset, \"[PAD]\"*28 + \"T\" + \"[MASK]\", \"A\")\n",
    "    check(2786358510 + offset, \"GTTAGGGTTAGGGTTAGGGTTAGGGTTAG\" + \"[MASK]\", \"G\")\n",
    "    print(len(\"GATTGGGATTGGGATTGGGA\")\n",
    "    check(2848818478 + offset, \"[PAD]\"*10 + \"CTAACCCTAACCCTAACCCT\" + \"[MASK]\", \"A\")\n",
    "\n",
    "    print(\"tests completed successfully!\")\n",
    "validate_T2T_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4f20da3f-e529-4db9-9ac5-980024002085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_idx: 5\n",
      "ttagggttagggttagggtt 20\n",
      "CACCCT 6\n",
      "tensor([4, 4, 4, 4, 8, 7, 8, 8, 8, 3]) tensor(10)\n",
      "local_idx: 5\n",
      "ttagggttagggttagggtt 20\n",
      "TTAGGGTTTA 10\n",
      "tensor([10, 10,  7,  9,  9,  9, 10, 10, 10,  3]) tensor(7)\n",
      "local_idx: 248387323\n",
      "ttagggttagggttagggtt 20\n",
      "TTAGGGTTAG 10\n",
      "tensor([10, 10,  7,  9,  9,  9, 10, 10,  7,  3]) tensor(9)\n",
      "local_idx: 248387323\n",
      "ttagggttagggttagggtt 20\n",
      "AACCC 5\n",
      "tensor([4, 4, 4, 4, 4, 7, 7, 8, 8, 3]) tensor(8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([4, 4, 4, 4, 4, 7, 7, 8, 8, 3]), tensor(8))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = CharacterTokenizer(\n",
    "    characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
    "    model_max_length=max_length + 2,  # to account for special tokens, like EOS\n",
    "    add_special_tokens=False,  # we handle special tokens elsewhere\n",
    "    padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
    ")\n",
    "\n",
    "dataset = GenomeDataset(fasta_path, tokenizer, 10, split=\"train\")\n",
    "dataset.__getitem__(5)\n",
    "dataset.__getitem__(dataset.__len__()//2 + 5)\n",
    "dataset.__getitem__(248387317)\n",
    "dataset.__getitem__(dataset.__len__()//2 + 248387317)\n",
    "# dataset.__getitem__(248387338)\n",
    "# dataset.__getitem__(2786358460)\n",
    "# dataset.__getitem__(1928464869)\n",
    "\n",
    "# tokenizer(\"ATGAAGGAAGG\", \n",
    "#                 add_special_tokens=False, \n",
    "#                 padding=\"max_length\",\n",
    "#                 max_length=10,\n",
    "#                 truncation=True,\n",
    "#             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff9a9d-df9c-42cc-b946-58cb4de6b1c0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecb0435f-d1a0-44a2-ace1-b29cfb926da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "GPU count: 8\n",
      "Current GPU: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "gpu = 4\n",
    "torch.cuda.set_device(gpu)\n",
    "print(\"Current GPU:\", torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348144e-b691-4ea6-8c0f-64c161124c78",
   "metadata": {},
   "source": [
    "## Investigations\n",
    "- Allocate inference cache? E.g., [here](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L142)\n",
    "- Init weights\n",
    "- **PARAMETERS**\n",
    "- Loss function? CrossEntropyLoss?\n",
    "- Optimizer? Adam?\n",
    "    - Note: HyenaDNA used AdamW\n",
    "### MambaTower\n",
    "- ~~no embedding like [here](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L149)? -> In MambaDNA~~\n",
    "- Do I need an embedding, for my small set of tokens?\n",
    "- ~~put nn.Linear behind? As in [MambaLMHeadModel](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L224) -> In MambaDNA~~\n",
    "### MambaBlock\n",
    "- [residual?](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L152)?\n",
    "- Different order: Original: Add -> LN -> MAMBA ([see](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L334-L350)); Here: MAMBA -> Add -> LN\n",
    "    - Does this remove the need for residual? Yes\n",
    "    - Is this equivalent?\n",
    "        - Close: MAMBA block is: `x = self.mamba(self.norm(x)) + x`\n",
    "        - Does it matter if residual connection does not include LN?\n",
    "    - Fuzed normalization (with add) used for higher performance ([see](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L311))\n",
    "    - Put LN in front (`x = self.mamba(self.norm(x)) + x`); would need to add normalization behind as well\n",
    "- Normalization: LayerNorm or RMSNorm?\n",
    "- Fuse normalization with add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0482a8b-d3f6-4b07-825d-a27c6f41870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://github.com/apapiu/mamba_small_bench\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout_level=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mamba = Mamba(d_model=embed_dim, d_state=16, d_conv=4, expand=2)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_level)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(self.mamba(x) + x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MambaTower(nn.Module):\n",
    "    def __init__(self, embed_dim, n_layers, seq_len=None, global_pool=False):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(*[MambaBlock(embed_dim) for _ in range(n_layers)])\n",
    "        self.global_pool = global_pool #for classification or other supervised learning.\n",
    "\n",
    "    def forward(self, x):\n",
    "        #for input (bs, n, d) it returns either (bs, n, d) or (bs, d) is global_pool\n",
    "        out = self.blocks(x) if not self.global_pool else torch.mean(self.blocks(x),1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MambaDNA(nn.Module):\n",
    "    def __init__(self, embed_dim, seq_len, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.tower = MambaTower(embed_dim, n_layers, seq_len=seq_len, global_pool=False)\n",
    "        self.out_proj = nn.Sequential(nn.LayerNorm(embed_dim),\n",
    "                                      nn.Linear(embed_dim, vocab_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tower(self.embed(x))\n",
    "        return self.out_proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46aff14-1adb-4841-8f69-19302c34d5db",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cb92b20-98dc-46f1-a8b2-51258a58ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1000\n",
    "\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = CharacterTokenizer(\n",
    "    characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
    "    model_max_length=max_length + 2,  # to account for special tokens, like EOS\n",
    "    add_special_tokens=False,  # we handle special tokens elsewhere\n",
    "    padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
