{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98235891-ea23-416a-b32d-58467e59d458",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Dataloader (pretraining)\n",
    "* [HyenaDNA HG38 dataloader](https://github.com/HazyResearch/hyena-dna/blob/main/src/dataloaders/datasets/hg38_dataset.py)\n",
    "* HyenaDNA used training/validation intervals from *Effective gene expression prediction from sequence by integrating long-range interactions.* paper.\n",
    "\n",
    "## Tokenizer?\n",
    "* Need to check HyenaDNA; I think their Jupyter contained some code of their tokenizer\n",
    "\n",
    "## Model\n",
    "* [Original MAMBA repo](https://github.com/state-spaces/mamba)\n",
    "    * [benchmark_generation_mamba_simple.py](https://github.com/state-spaces/mamba/blob/main/benchmarks/benchmark_generation_mamba_simple.py)\n",
    "    * Uses [mambaLMHeadModel](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L173) form `mixer_seq_simple.py`\n",
    "    * Uses [MixerModel](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L83)\n",
    "    * Uses [create_block](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L21)\n",
    "    * Uses [Block](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L298) and [MAMBA](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L34) classes (from `mamba_simple.py`)\n",
    "        * Actual MAMBA operation: [mamba_inner_fn](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/ops/selective_scan_interface.py#L155)\n",
    "* [Mamba small benchmark repo](https://github.com/apapiu/mamba_small_bench)\n",
    "* [SimplerMambaSSM Jupyter Notebook](./SimplerMambaSSM.ipynb)\n",
    "    * Use mamba-ssm library\n",
    "    * See class BigNeuralNetwork\n",
    "* [MAMBA chat](https://github.com/havenhq/mamba-chat/blob/main/train_mamba.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c3297-f67c-4ac8-8403-f98a60637cc9",
   "metadata": {},
   "source": [
    "# Required python packages\n",
    "1. PyTorch (with CUDA)\n",
    "2. mamba-ssm==1.0.1\n",
    "3. transformers==4.26.1 *(for tokenizer)*\n",
    "4. causal-conv1d==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75af3307-1af8-4faf-b32b-99de0382706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "from pyfaidx import Fasta\n",
    "import pynvml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec1144b-3e2a-4cbd-9a73-db90afc31f48",
   "metadata": {},
   "source": [
    "# Download genetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f33b3a-1046-4a84-9d74-4cf1d5cc0b44",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_genetic_data():\n",
    "    # datasets\n",
    "    hg38_url = 'https://api.ncbi.nlm.nih.gov/datasets/v2alpha/genome/accession/GCF_000001405.40/download'\n",
    "    t2t_url = 'https://api.ncbi.nlm.nih.gov/datasets/v2alpha/genome/accession/GCF_009914755.1/download'\n",
    "    dataset_url = t2t_url\n",
    "    \n",
    "    print(\"download started...\")\n",
    "    response = requests.get(dataset_url, params={'include_annotation_type': 'GENOME_FASTA'})\n",
    "    if response.status_code == 200:\n",
    "        data_dir_path = 'dataset'\n",
    "        os.makedirs(data_dir_path, exist_ok=True)\n",
    "        with BytesIO(response.content) as zip_buffer:\n",
    "            ZipFile(zip_buffer, 'r').extractall(path=data_dir_path)\n",
    "        print(\"dataset ready\")\n",
    "    \n",
    "    gh38_fasta = 'dataset/ncbi_dataset/data/GCF_000001405.40/GCF_000001405.40_GRCh38.p14_genomic.fna'\n",
    "    \n",
    "    print(\"FASTA files:\")\n",
    "    fpaths = list(Path('dataset').rglob('*.fna'))\n",
    "    for fpath in fpaths:\n",
    "        print(fpath)\n",
    "    \n",
    "    fasta_path = fpaths[0]\n",
    "\n",
    "# download_genetic_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac19307-d524-4eb1-af22-75ad8eed139c",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0071b7-7054-47db-bb69-00ee4d964980",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HyenaDNA tokenizer; code from their jupyter notebook\n",
    "\"\"\"\n",
    "Just a simple character level tokenizer.\n",
    "\n",
    "From: https://github.com/dariush-bahrami/character-tokenizer/blob/master/charactertokenizer/core.py\n",
    "\n",
    "CharacterTokenzier for Hugging Face Transformers.\n",
    "This is heavily inspired from CanineTokenizer in transformers package.\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class CharacterTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token called\n",
    "                [UNK] with id=6. Following are list of all of the special tokens with\n",
    "                their corresponding ids:\n",
    "                    \"[CLS]\": 0\n",
    "                    \"[SEP]\": 1\n",
    "                    \"[BOS]\": 2\n",
    "                    \"[MASK]\": 3\n",
    "                    \"[PAD]\": 4\n",
    "                    \"[RESERVED]\": 5\n",
    "                    \"[UNK]\": 6\n",
    "                an id (starting at 7) will be assigned to each character.\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "\n",
    "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=sep_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_side=padding_side,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def convert_token_vector_to_string(self, ivector):\n",
    "        out_str = \"\"\n",
    "        for i in ivector:\n",
    "            out_str = out_str + self._convert_id_to_token(i.item())\n",
    "        return out_str\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        result = cls + token_ids_0 + sep\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "        return result\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf372c-23c9-4f40-b25d-9c617abeb126",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3d2ec6e-8035-4d95-a49a-18753c25c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement(in_seq):\n",
    "    out_seq = \"\"\n",
    "    for idx, c in enumerate(in_seq):\n",
    "        oc = \"X\"\n",
    "        if c == 'A':\n",
    "            oc = 'T'\n",
    "        elif c == 'T':\n",
    "            oc = 'A'\n",
    "        elif c == 'C':\n",
    "            oc = 'G'\n",
    "        elif c == 'G':\n",
    "            oc = 'C'\n",
    "        elif c == 'N':\n",
    "            oc = 'N'\n",
    "        else:\n",
    "            assert True == False\n",
    "        out_seq = out_seq + oc\n",
    "    return out_seq\n",
    "    \n",
    "\n",
    "class GenomeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, fasta_path, ds_entries):\n",
    "        assert Path(fasta_path).exists\n",
    "        self.fasta = Fasta(fasta_path, one_based_attributes=False)\n",
    "\n",
    "        dtype = np.dtype([('key', 'U20'), ('start', 'int_'), ('end', 'int_')])\n",
    "        self.entry_ranges = np.empty(len(ds_entries), dtype=dtype)\n",
    "\n",
    "        # only append entries of dataset\n",
    "        count = 0\n",
    "        for idx, k in enumerate(ds_entries):\n",
    "            assert k in self.fasta.keys(), \\\n",
    "                \"FASTA file does not contain an entry with key {}\".format(k)\n",
    "            seq_len = len(self.fasta[k])\n",
    "            self.entry_ranges[idx] = np.array([(k, count, count + seq_len)], dtype=dtype)\n",
    "            count = count + seq_len\n",
    "\n",
    "        # for e in self.entry_ranges:\n",
    "        #     print(e)\n",
    "\n",
    "    def config(self, tokenizer, seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "             \n",
    "    def __len__(self):\n",
    "        # first range forward idices, second range reverse complement\n",
    "        return self.entry_ranges[-1]['end'] * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx >= 0\n",
    "        assert idx < self.__len__()\n",
    "\n",
    "        # return reverse complement?\n",
    "        rev_compl = (idx >= self.entry_ranges[-1]['end'])\n",
    "        idx = idx % self.entry_ranges[-1]['end']\n",
    "\n",
    "        # locate FASTA entry of global idx\n",
    "        key = None\n",
    "        local_idx = -1\n",
    "        for e in self.entry_ranges:\n",
    "            if e['start'] <= idx < e['end']:\n",
    "                key = e['key']\n",
    "                local_idx = idx - e['start']\n",
    "\n",
    "        assert key != None\n",
    "        assert local_idx != -1\n",
    "        # print(\"local_idx: {}, rev_compl: {}\".format(local_idx, rev_compl))\n",
    "\n",
    "        left_bound = 0\n",
    "        right_bound = len(self.fasta[key])\n",
    "\n",
    "        # print(self.fasta[key][-20:], len(self.fasta[key][-20:]))\n",
    "\n",
    "        seq = None\n",
    "        if not(rev_compl):\n",
    "            seq = self.fasta[key][:local_idx + 1][-self.seq_len:]\n",
    "        else:\n",
    "            seq = self.fasta[key][local_idx:][::-1][-self.seq_len:]\n",
    "        assert seq != None\n",
    "\n",
    "        # capitalize all nucleotides\n",
    "        seq_str = str(seq).upper()\n",
    "        # print(\"seq_str: {}\".format(seq_str))\n",
    "\n",
    "        # use complement when reverse\n",
    "        if rev_compl:\n",
    "            seq_str = complement(seq_str)\n",
    "        # print(seq_str, len(seq_str))\n",
    "        assert len(seq_str) <= self.seq_len\n",
    "\n",
    "        tokens = self.tokenizer(seq_str, add_special_tokens=False, padding=\"max_length\",\n",
    "                                max_length=self.seq_len, truncation=True)\n",
    "        input = torch.LongTensor(tokens[\"input_ids\"]).clone()\n",
    "        # print(\"input: {}\".format(input))\n",
    "\n",
    "        # mask\n",
    "        target = input.clone()\n",
    "        input[-1] = self.tokenizer._vocab_str_to_int['[MASK]']\n",
    "        # print(input, target)\n",
    "        return input, target\n",
    "\n",
    "def get_T2T_datasets():\n",
    "    T2T_path = \"dataset/ncbi_dataset/data/GCF_009914755.1/GCF_009914755.1_T2T-CHM13v2.0_genomic.fna\"\n",
    "    training_entries = ['NC_060925.1', 'NC_060926.1', 'NC_060927.1', 'NC_060928.1', 'NC_060929.1',\n",
    "                        'NC_060931.1', 'NC_060932.1', 'NC_060933.1', 'NC_060934.1', 'NC_060935.1',\n",
    "                        'NC_060936.1', 'NC_060937.1', 'NC_060938.1', 'NC_060939.1', 'NC_060941.1',\n",
    "                        'NC_060942.1', 'NC_060943.1', 'NC_060944.1', 'NC_060945.1', 'NC_060946.1',\n",
    "                        'NC_060947.1', 'NC_060948.1']\n",
    "    test_entries = ['NC_060930.1', 'NC_060940.1']\n",
    "\n",
    "    # check training and test dataset do not contain the same entries\n",
    "    assert set(training_entries).isdisjoint(set(test_entries)) == True\n",
    "    \n",
    "    train_dataset = GenomeDataset(T2T_path, training_entries)\n",
    "    test_dataset = GenomeDataset(T2T_path, test_entries)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# train_ds, test_ds = get_T2T_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e201da3-ea6f-404d-b862-a62f28508f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NC_060925.1',          0,  248387328)\n",
      " ('NC_060926.1',  248387328,  491084080)\n",
      " ('NC_060927.1',  491084080,  692190028)\n",
      " ('NC_060928.1',  692190028,  885764973)\n",
      " ('NC_060929.1',  885764973, 1067810412)\n",
      " ('NC_060931.1', 1067810412, 1228377840)\n",
      " ('NC_060932.1', 1228377840, 1374637171)\n",
      " ('NC_060933.1', 1374637171, 1525254418)\n",
      " ('NC_060934.1', 1525254418, 1660012552)\n",
      " ('NC_060935.1', 1660012552, 1795140321)\n",
      " ('NC_060936.1', 1795140321, 1928464869)\n",
      " ('NC_060937.1', 1928464869, 2042031555)\n",
      " ('NC_060938.1', 2042031555, 2143193047)\n",
      " ('NC_060939.1', 2143193047, 2242946242)\n",
      " ('NC_060941.1', 2242946242, 2327223139)\n",
      " ('NC_060942.1', 2327223139, 2407765677)\n",
      " ('NC_060943.1', 2407765677, 2469473041)\n",
      " ('NC_060944.1', 2469473041, 2535683296)\n",
      " ('NC_060945.1', 2535683296, 2580773978)\n",
      " ('NC_060946.1', 2580773978, 2632098904)\n",
      " ('NC_060947.1', 2632098904, 2786358470)\n",
      " ('NC_060948.1', 2786358470, 2848818499)]\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tokens match!\n",
      "tests completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# validate results of T2T dataset;\n",
    "# check if correct tokens are returned for predefined indices\n",
    "def validate_T2T_ds():\n",
    "    train_ds, _ = get_T2T_datasets()\n",
    "    \n",
    "    token_len = 30\n",
    "    tokenizer = CharacterTokenizer(\n",
    "        characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
    "        model_max_length=token_len,  # to account for special tokens, like EOS\n",
    "        add_special_tokens=False,  # we handle special tokens elsewhere\n",
    "        padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
    "    )\n",
    "\n",
    "    train_ds.config(tokenizer, token_len)\n",
    "    print(train_ds.entry_ranges)\n",
    "    \n",
    "    def check(idx, expct_inpt, expct_trgt):\n",
    "        inpt, trgt = train_ds.__getitem__(idx)\n",
    "        actual_trgt = tokenizer.convert_token_vector_to_string(trgt)\n",
    "        actual_inpt = tokenizer.convert_token_vector_to_string(inpt)\n",
    "\n",
    "        # Does not recognize \"PAD\" and \"MASK\" token\n",
    "        # expct_inpt_len = len(tokenizer(expct_inpt, add_special_tokens=False, max_length=token_len,\n",
    "        #                            truncation=False)[\"input_ids\"])\n",
    "        # assert expct_inpt_len == token_len, \\\n",
    "        #     \"Unexpected token length of expct_inpt ({})\".format(expct_inpt_len)\n",
    "        \n",
    "        assert expct_trgt == actual_trgt, \\\n",
    "            \"Target tokens do not match; expected: {} (len {}), actual: {} (len {})\".format(expct_trgt, len(expct_trgt), actual_trgt, len(actual_trgt))\n",
    "        assert expct_inpt == actual_inpt, \\\n",
    "            \"Input tokens do not match; expected: {} (len {}), actual: {} (len {})\".format(expct_inpt, len(expct_inpt), actual_inpt, len(actual_inpt))\n",
    "        print(\"tokens match!\")\n",
    "\n",
    "    # forward\n",
    "    check(5, \"[PAD]\"*24 + \"CACCC\" + \"[MASK]\", \"[PAD]\"*24 + \"CACCC\" + \"T\")\n",
    "    check(248387322, \"GGGTTAGGGTTAGGGTTAGGGTTAGGGTT\" + \"[MASK]\", \"GGGTTAGGGTTAGGGTTAGGGTTAGGGTT\" + \"A\")\n",
    "    check(1067810436, \"[PAD]\"*5 + \"CCTAACCCTAACCCTAACCCCTAA\" + \"[MASK]\", \"[PAD]\"*5 + \"CCTAACCCTAACCCTAACCCCTAA\" + \"C\")\n",
    "    check(1228377838, \"GGGTTAGGGTTAGGGGTTAGGGTTAGGGT\" + \"[MASK]\", \"GGGTTAGGGTTAGGGGTTAGGGTTAGGGT\" + \"T\")\n",
    "    check(2786358510, \"CTAACCCTAACCCTAACCCTAACCCTAAC\" + \"[MASK]\", \"CTAACCCTAACCCTAACCCTAACCCTAAC\" + \"C\")\n",
    "    check(2848818478, \"AGGGTTAGGGTTAGGGTTAGGGTTAGGGT\" + \"[MASK]\", \"AGGGTTAGGGTTAGGGTTAGGGTTAGGGT\" + \"T\")\n",
    "    # reverse complement\n",
    "    offset = 2848818499\n",
    "    check((5 + offset), \"GTTAGGGTTAGGGTTAGGGGTTAGGGTTT\" + \"[MASK]\", \"GTTAGGGTTAGGGTTAGGGGTTAGGGTTT\" + \"A\")\n",
    "    check(248387322 + offset, \"[PAD]\"*24 + \"AACCC\" + \"[MASK]\", \"[PAD]\"*24 + \"AACCC\" + \"T\")\n",
    "    check(1067810436 + offset, \"GTTAGGAGGGTTAGGGGATTAGGGTTAGG\" + \"[MASK]\", \"GTTAGGAGGGTTAGGGGATTAGGGTTAGG\" + \"G\")\n",
    "    check(1228377838 + offset, \"[PAD]\"*28 + \"T\" + \"[MASK]\", \"[PAD]\"*28 + \"T\" + \"A\")\n",
    "    check(2786358510 + offset, \"GTTAGGGTTAGGGTTAGGGTTAGGGTTAG\" + \"[MASK]\", \"GTTAGGGTTAGGGTTAGGGTTAGGGTTAG\" + \"G\")\n",
    "    check(2848818478 + offset, \"[PAD]\"*9 + \"CTAACCCTAACCCTAACCCT\" + \"[MASK]\", \"[PAD]\"*9 + \"CTAACCCTAACCCTAACCCT\" + \"A\")\n",
    "\n",
    "    print(\"tests completed successfully!\")\n",
    "validate_T2T_ds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff9a9d-df9c-42cc-b946-58cb4de6b1c0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348144e-b691-4ea6-8c0f-64c161124c78",
   "metadata": {},
   "source": [
    "## Investigations\n",
    "- Allocate inference cache? E.g., [here](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L142)\n",
    "- Init weights\n",
    "- **PARAMETERS**\n",
    "- Loss function? CrossEntropyLoss?\n",
    "- ~~Optimizer? Adam?~~\n",
    "    - ~~Note: HyenaDNA used AdamW~~\n",
    "- Sequence-length warm-up\n",
    "### MambaTower\n",
    "- ~~no embedding like [here](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L149)? -> In MambaDNA~~\n",
    "- Do I need an embedding, for my small set of tokens?\n",
    "- ~~put nn.Linear behind? As in [MambaLMHeadModel](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L224) -> In MambaDNA~~\n",
    "### MambaBlock\n",
    "- [residual?](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/models/mixer_seq_simple.py#L152)?\n",
    "- Different order: Original: Add -> LN -> MAMBA ([see](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L334-L350)); Here: MAMBA -> Add -> LN\n",
    "    - Does this remove the need for residual? Yes\n",
    "    - Is this equivalent?\n",
    "        - Close: MAMBA block is: `x = self.mamba(self.norm(x)) + x`\n",
    "        - Does it matter if residual connection does not include LN?\n",
    "    - Fuzed normalization (with add) used for higher performance ([see](https://github.com/state-spaces/mamba/blob/bae8d1a42fec58f4cdd300bf3b987d05eab22ed0/mamba_ssm/modules/mamba_simple.py#L311))\n",
    "    - Put LN in front (`x = self.mamba(self.norm(x)) + x`); would need to add normalization behind as well\n",
    "- Normalization: LayerNorm or RMSNorm?\n",
    "- Fuse normalization with add\n",
    "\n",
    "## Notes\n",
    "- Optimizer: AdamW with (𝛽1 , 𝛽2 ) = (0.9, 0.95); same as Mamba paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0482a8b-d3f6-4b07-825d-a27c6f41870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://github.com/apapiu/mamba_small_bench\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout_level=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mamba = Mamba(d_model=embed_dim, d_state=16, d_conv=4, expand=2)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_level)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(self.mamba(x) + x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MambaTower(nn.Module):\n",
    "    def __init__(self, embed_dim, n_layers, seq_len=None, dropout=0, global_pool=False):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(*[MambaBlock(embed_dim, dropout) for _ in range(n_layers)])\n",
    "        self.global_pool = global_pool #for classification or other supervised learning.\n",
    "\n",
    "    def forward(self, x):\n",
    "        #for input (bs, n, d) it returns either (bs, n, d) or (bs, d) is global_pool\n",
    "        out = self.blocks(x) if not self.global_pool else torch.mean(self.blocks(x),1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MambaDNA(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, seq_len, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.tower = MambaTower(embed_dim, n_layers, seq_len=seq_len,\n",
    "                                dropout=dropout, global_pool=False)\n",
    "        self.out_proj = nn.Sequential(nn.LayerNorm(embed_dim),\n",
    "                                      nn.Linear(embed_dim, vocab_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tower(self.embed(x))\n",
    "        return self.out_proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46aff14-1adb-4841-8f69-19302c34d5db",
   "metadata": {},
   "source": [
    "# Pretraining\n",
    "## TODO\n",
    "- Improve multi-GPU training\n",
    "- Init weights\n",
    "- Allocate inference cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb92b20-98dc-46f1-a8b2-51258a58ff9a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2,3,4,5,6,7\n",
      "Using 6 GPUs: NVIDIA RTX 6000 Ada Generation\n",
      "Not implemented: Model load/store\n",
      "#703756 model parameters\n",
      "step 0-0: Loss 2.505395; Masked prediction accuracy 0.0000%; elapsed time 956.10 seconds\n",
      "step 0-1: Loss 0.147665; Masked prediction accuracy 19.4336%; elapsed time 898.89 seconds\n",
      "step 0-2: Loss 0.015071; Masked prediction accuracy 29.1016%; elapsed time 952.52 seconds\n",
      "step 0-3: Loss 0.013165; Masked prediction accuracy 28.7109%; elapsed time 999.92 seconds\n",
      "step 0-4: Loss 0.011922; Masked prediction accuracy 30.3711%; elapsed time 923.43 seconds\n",
      "step 0-5: Loss 0.010061; Masked prediction accuracy 24.0234%; elapsed time 924.25 seconds\n",
      "step 0-6: Loss 0.008449; Masked prediction accuracy 31.0547%; elapsed time 872.44 seconds\n",
      "step 0-7: Loss 0.006637; Masked prediction accuracy 29.8828%; elapsed time 945.73 seconds\n",
      "step 0-8: Loss 0.011313; Masked prediction accuracy 29.0039%; elapsed time 735.25 seconds\n",
      "step 0-9: Loss 0.005465; Masked prediction accuracy 25.1953%; elapsed time 720.26 seconds\n",
      "step 0-10: Loss 0.005341; Masked prediction accuracy 28.0273%; elapsed time 840.94 seconds\n",
      "step 0-11: Loss 0.004612; Masked prediction accuracy 23.5352%; elapsed time 1107.81 seconds\n",
      "step 0-12: Loss 0.003824; Masked prediction accuracy 26.0742%; elapsed time 1056.69 seconds\n",
      "step 0-13: Loss 0.003219; Masked prediction accuracy 31.4453%; elapsed time 963.36 seconds\n",
      "step 0-14: Loss 0.003059; Masked prediction accuracy 24.4141%; elapsed time 850.42 seconds\n",
      "step 0-15: Loss 0.002365; Masked prediction accuracy 24.1211%; elapsed time 918.81 seconds\n",
      "step 0 test: Masked prediction accuracy 25.0000%\n",
      "step 1-0: Loss 0.003430; Masked prediction accuracy 31.4453%; elapsed time 819.22 seconds\n",
      "step 1-1: Loss 0.002140; Masked prediction accuracy 28.5156%; elapsed time 918.49 seconds\n",
      "step 1-2: Loss 0.002088; Masked prediction accuracy 31.7383%; elapsed time 822.08 seconds\n",
      "step 1-3: Loss 0.002169; Masked prediction accuracy 30.8594%; elapsed time 815.67 seconds\n",
      "step 1-4: Loss 0.002188; Masked prediction accuracy 29.3945%; elapsed time 821.70 seconds\n",
      "step 1-5: Loss 0.001964; Masked prediction accuracy 29.3945%; elapsed time 1078.20 seconds\n",
      "step 1-6: Loss 0.002205; Masked prediction accuracy 26.2695%; elapsed time 875.79 seconds\n",
      "step 1-7: Loss 0.001722; Masked prediction accuracy 29.3945%; elapsed time 877.22 seconds\n",
      "step 1-8: Loss 0.001686; Masked prediction accuracy 26.3672%; elapsed time 885.44 seconds\n",
      "step 1-9: Loss 0.001647; Masked prediction accuracy 26.4648%; elapsed time 973.65 seconds\n",
      "step 1-10: Loss 0.001597; Masked prediction accuracy 29.6875%; elapsed time 918.04 seconds\n",
      "step 1-11: Loss 0.001547; Masked prediction accuracy 33.3984%; elapsed time 888.60 seconds\n",
      "step 1-12: Loss 0.001514; Masked prediction accuracy 31.9336%; elapsed time 892.48 seconds\n",
      "step 1-13: Loss 0.001503; Masked prediction accuracy 29.7852%; elapsed time 959.43 seconds\n",
      "step 1-14: Loss 0.001464; Masked prediction accuracy 32.3242%; elapsed time 835.92 seconds\n",
      "step 1-15: Loss 0.001416; Masked prediction accuracy 32.7148%; elapsed time 828.23 seconds\n",
      "step 1 test: Masked prediction accuracy 39.0625%\n",
      "step 2-0: Loss 0.001438; Masked prediction accuracy 34.3750%; elapsed time 829.05 seconds\n",
      "step 2-1: Loss 0.001454; Masked prediction accuracy 30.1758%; elapsed time 815.31 seconds\n",
      "step 2-2: Loss 0.001406; Masked prediction accuracy 30.5664%; elapsed time 839.40 seconds\n",
      "step 2-3: Loss 0.001384; Masked prediction accuracy 34.6680%; elapsed time 924.21 seconds\n",
      "step 2-4: Loss 0.001399; Masked prediction accuracy 32.5195%; elapsed time 976.90 seconds\n",
      "step 2-5: Loss 0.001389; Masked prediction accuracy 33.3984%; elapsed time 857.75 seconds\n",
      "step 2-6: Loss 0.001368; Masked prediction accuracy 33.8867%; elapsed time 856.40 seconds\n",
      "step 2-7: Loss 0.001376; Masked prediction accuracy 34.2773%; elapsed time 941.03 seconds\n",
      "step 2-8: Loss 0.001383; Masked prediction accuracy 32.2266%; elapsed time 853.35 seconds\n",
      "step 2-9: Loss 0.001372; Masked prediction accuracy 32.6172%; elapsed time 866.41 seconds\n",
      "step 2-10: Loss 0.001346; Masked prediction accuracy 35.1562%; elapsed time 844.36 seconds\n",
      "step 2-11: Loss 0.001361; Masked prediction accuracy 33.0078%; elapsed time 934.40 seconds\n",
      "step 2-12: Loss 0.001367; Masked prediction accuracy 32.9102%; elapsed time 858.18 seconds\n",
      "step 2-13: Loss 0.001348; Masked prediction accuracy 34.7656%; elapsed time 853.36 seconds\n",
      "step 2-14: Loss 0.001343; Masked prediction accuracy 32.0312%; elapsed time 864.23 seconds\n",
      "step 2-15: Loss 0.001341; Masked prediction accuracy 35.4492%; elapsed time 953.52 seconds\n",
      "step 2 test: Masked prediction accuracy 31.2500%\n",
      "step 3-0: Loss 0.001313; Masked prediction accuracy 37.2070%; elapsed time 960.07 seconds\n",
      "step 3-1: Loss 0.001323; Masked prediction accuracy 35.4492%; elapsed time 863.85 seconds\n",
      "step 3-2: Loss 0.001358; Masked prediction accuracy 33.3984%; elapsed time 850.58 seconds\n",
      "step 3-3: Loss 0.001347; Masked prediction accuracy 34.0820%; elapsed time 859.44 seconds\n",
      "step 3-4: Loss 0.001330; Masked prediction accuracy 34.1797%; elapsed time 925.68 seconds\n",
      "step 3-5: Loss 0.001313; Masked prediction accuracy 37.5000%; elapsed time 852.49 seconds\n",
      "step 3-6: Loss 0.001325; Masked prediction accuracy 33.1055%; elapsed time 851.24 seconds\n",
      "step 3-7: Loss 0.001324; Masked prediction accuracy 35.7422%; elapsed time 870.42 seconds\n",
      "step 3-8: Loss 0.001336; Masked prediction accuracy 32.9102%; elapsed time 849.76 seconds\n",
      "step 3-9: Loss 0.001329; Masked prediction accuracy 35.8398%; elapsed time 922.45 seconds\n",
      "step 3-10: Loss 0.001323; Masked prediction accuracy 34.8633%; elapsed time 857.17 seconds\n",
      "step 3-11: Loss 0.001322; Masked prediction accuracy 36.5234%; elapsed time 888.16 seconds\n",
      "step 3-12: Loss 0.001318; Masked prediction accuracy 36.6211%; elapsed time 885.12 seconds\n",
      "step 3-13: Loss 0.001330; Masked prediction accuracy 37.2070%; elapsed time 1095.69 seconds\n",
      "step 3-14: Loss 0.001328; Masked prediction accuracy 33.8867%; elapsed time 841.49 seconds\n",
      "step 3-15: Loss 0.001337; Masked prediction accuracy 34.8633%; elapsed time 886.15 seconds\n",
      "step 3 test: Masked prediction accuracy 32.8125%\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2,3,4,5,6,7\n",
    "\n",
    "assert torch.cuda.is_available() == True, \"CUDA unavailable\"\n",
    "device = torch.device('cuda')\n",
    "# torch.cuda.set_device(device)\n",
    "# assert torch.cuda.utilization() == 0, \"GPU in use!\"\n",
    "print(\"Using {} GPUs: {}\".format(torch.cuda.device_count(), torch.cuda.get_device_name()))\n",
    "\n",
    "\n",
    "# parameters\n",
    "embed_dim = 128\n",
    "n_layers = 6\n",
    "dropout = 0             # original Mamba did not use dropout\n",
    "# training\n",
    "# reproducing sec 4.3.2 with 1.3-1.4M parameters, 330B token pretraining\n",
    "seq_len = 1024\n",
    "batch_size_train = 1024\n",
    "batches_per_step = 16\n",
    "batch_size_test = 64\n",
    "n_steps = 5 # 20000\n",
    "# optimizer\n",
    "lr = 8e-3\n",
    "# epsilon = 0.2 # ???\n",
    "weight_decay = 0.1\n",
    "\n",
    "model_state_dir = \"models\"\n",
    "model_state_path = model_state_dir + \"/\" + \"12-28-2023-1\"\n",
    "\n",
    "\n",
    "train_ds, test_ds = get_T2T_datasets()\n",
    "\n",
    "tokenizer = CharacterTokenizer(\n",
    "    characters=['A', 'C', 'G', 'T', 'N'],\n",
    "    model_max_length=seq_len,\n",
    "    add_special_tokens=False,\n",
    "    padding_side='left',\n",
    ")\n",
    "train_ds.config(tokenizer, seq_len)\n",
    "test_ds.config(tokenizer, seq_len)\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size_train, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "model = MambaDNA(vocab_size=tokenizer.vocab_size, embed_dim=embed_dim,\n",
    "                 seq_len=seq_len, n_layers=n_layers, dropout=dropout)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "print(\"Not implemented: Model load/store\")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95),\n",
    "                              weight_decay=weight_decay) #eps=epsilon, \n",
    "\n",
    "# number of parameters of model\n",
    "print(\"#{} model parameters\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "# experimenting\n",
    "# n_steps = 1\n",
    "# batches_per_step = 5\n",
    "\n",
    "for train_idx in range(n_steps):\n",
    "    # compute next-token prediction accuracy\n",
    "    def comp_next_token_pred_acc(prediction, target):\n",
    "        prediction = prediction[:,-1,:]\n",
    "        target = target[:,-1]\n",
    "        _, pred_labels = torch.max(prediction, 1)\n",
    "        corr_pred = (pred_labels == target)\n",
    "        accuracy = corr_pred.sum().item() / target.size(-1)\n",
    "        return accuracy\n",
    "        \n",
    "    # training\n",
    "    for b_idx in range(batches_per_step):\n",
    "        stime = time.time()\n",
    "        # TODO empty dataloader?\n",
    "        inpts, trgts = next(iter(train_dataloader))\n",
    "        inpts, trgts = inpts.to(device), trgts.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outpts = model(inpts)\n",
    "\n",
    "        loss = loss_fn(outpts.view(-1, outpts.size(-1)), trgts.view(-1))\n",
    "        loss_val = loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        etime = time.time()\n",
    "\n",
    "        accuracy = comp_next_token_pred_acc(outpts, trgts)\n",
    "        print(\"step {}-{}: Loss {:.6f}; Masked prediction accuracy {:.4f}%; elapsed time {:.2f} seconds\".format(train_idx, b_idx, loss.item(), accuracy*100.0, etime-stime))\n",
    "    \n",
    "    # testing\n",
    "    inpts, trgts = next(iter(test_dataloader))\n",
    "    inpts, trgts = inpts.to(device), trgts.to(device)\n",
    "\n",
    "    outpts = model(inpts)\n",
    "    accuracy = comp_next_token_pred_acc(outpts, trgts)\n",
    "    print(\"step {} test: Masked prediction accuracy {:.4f}%\".format(train_idx, accuracy*100.0))\n",
    "\n",
    "    os.makedirs(model_state_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), model_state_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
